import os
import json
from typing import List, Dict, Any
from pathlib import Path

from dotenv import load_dotenv
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_deepseek import ChatDeepSeek
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import (
    TextFileLoader, 
    PyPDFLoader, 
    CSVLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredMarkdownLoader
)
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class DocumentKnowledgeBase:
    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        self.llm = ChatDeepSeek(
            api_key=os.getenv("DEEPSEEK_API_KEY"), 
            model="deepseek-chat",
            temperature=0.1
        )
        
        # ä½¿ç”¨å…è´¹çš„HuggingFaceåµŒå…¥æ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
        
        self.vectorstore = None
        self.documents = []
        
        # åˆ›å»ºå­˜å‚¨ç›®å½•
        self.storage_dir = Path("knowledge_base")
        self.storage_dir.mkdir(exist_ok=True)
        
        # åŠ è½½å·²æœ‰çš„çŸ¥è¯†åº“
        self.load_existing_knowledge_base()
    
    def load_document(self, file_path: str) -> List[Document]:
        """æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_extension = file_path.suffix.lower()
        
        try:
            if file_extension == '.txt':
                loader = TextFileLoader(str(file_path), encoding='utf-8')
            elif file_extension == '.pdf':
                loader = PyPDFLoader(str(file_path))
            elif file_extension == '.csv':
                loader = CSVLoader(str(file_path))
            elif file_extension in ['.doc', '.docx']:
                loader = UnstructuredWordDocumentLoader(str(file_path))
            elif file_extension == '.md':
                loader = UnstructuredMarkdownLoader(str(file_path))
            else:
                # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶å¤„ç†
                loader = TextFileLoader(str(file_path), encoding='utf-8')
            
            documents = loader.load()
            
            # ä¸ºæ–‡æ¡£æ·»åŠ æ¥æºä¿¡æ¯
            for doc in documents:
                doc.metadata['source'] = str(file_path)
                doc.metadata['filename'] = file_path.name
            
            return documents
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
    
    def add_documents(self, file_paths: List[str]):
        """æ·»åŠ å¤šä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        new_documents = []
        
        for file_path in file_paths:
            print(f"ğŸ“„ æ­£åœ¨åŠ è½½: {file_path}")
            docs = self.load_document(file_path)
            if docs:
                new_documents.extend(docs)
                print(f"âœ… æˆåŠŸåŠ è½½: {file_path} ({len(docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ)")
            else:
                print(f"âŒ åŠ è½½å¤±è´¥: {file_path}")
        
        if new_documents:
            # åˆ†å‰²æ–‡æ¡£
            print("ğŸ“ æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
            split_docs = self.text_splitter.split_documents(new_documents)
            
            # æ·»åŠ åˆ°ç°æœ‰æ–‡æ¡£åˆ—è¡¨
            self.documents.extend(split_docs)
            
            # æ›´æ–°æˆ–åˆ›å»ºå‘é‡å­˜å‚¨
            if self.vectorstore is None:
                print("ğŸ” æ­£åœ¨åˆ›å»ºå‘é‡ç´¢å¼•...")
                self.vectorstore = FAISS.from_documents(split_docs, self.embeddings)
            else:
                print("ğŸ” æ­£åœ¨æ›´æ–°å‘é‡ç´¢å¼•...")
                self.vectorstore.add_documents(split_docs)
            
            # ä¿å­˜çŸ¥è¯†åº“
            self.save_knowledge_base()
            
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(split_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°çŸ¥è¯†åº“")
            print(f"ğŸ“Š çŸ¥è¯†åº“æ€»æ–‡æ¡£æ•°: {len(self.documents)}")
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
    
    def save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°æœ¬åœ°"""
        try:
            if self.vectorstore:
                # ä¿å­˜FAISSç´¢å¼•
                faiss_path = self.storage_dir / "faiss_index"
                self.vectorstore.save_local(str(faiss_path))
                
                # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
                doc_metadata = []
                for doc in self.documents:
                    doc_metadata.append({
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    })
                
                metadata_path = self.storage_dir / "documents_metadata.json"
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(doc_metadata, f, ensure_ascii=False, indent=2)
                
                print("ğŸ’¾ çŸ¥è¯†åº“å·²ä¿å­˜åˆ°æœ¬åœ°")
        except Exception as e:
            print(f"âŒ ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {e}")
    
    def load_existing_knowledge_base(self):
        """åŠ è½½å·²æœ‰çš„çŸ¥è¯†åº“"""
        try:
            faiss_path = self.storage_dir / "faiss_index"
            metadata_path = self.storage_dir / "documents_metadata.json"
            
            if faiss_path.exists() and metadata_path.exists():
                # åŠ è½½FAISSç´¢å¼•
                self.vectorstore = FAISS.load_local(
                    str(faiss_path), 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                
                # åŠ è½½æ–‡æ¡£å…ƒæ•°æ®
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    doc_metadata = json.load(f)
                
                self.documents = [
                    Document(page_content=item['content'], metadata=item['metadata'])
                    for item in doc_metadata
                ]
                
                print(f"ğŸ“š å·²åŠ è½½ç°æœ‰çŸ¥è¯†åº“ï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç°æœ‰çŸ¥è¯†åº“å¤±è´¥: {e}")
    
    def search_documents(self, query: str, k: int = 5) -> List[Document]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.vectorstore:
            return []
        
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            print(f"âŒ æœç´¢æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    def query(self, question: str) -> str:
        """åŸºäºçŸ¥è¯†åº“å›ç­”é—®é¢˜"""
        if not self.vectorstore:
            return "âŒ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£ã€‚"
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.search_documents(question, k=3)
        
        if not relevant_docs:
            return "âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£ç‰‡æ®µ {i+1} (æ¥æº: {doc.metadata.get('filename', 'æœªçŸ¥')}):\n{doc.page_content}"
            for i, doc in enumerate(relevant_docs)
        ])
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt_template = PromptTemplate.from_template("""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚è¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ä¸”åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ã€‚

æ–‡æ¡£å†…å®¹:
{context}

é—®é¢˜: {question}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
å›ç­”:""")
        
        # åˆ›å»ºé“¾å¹¶æ‰§è¡Œ
        chain = LLMChain(llm=self.llm, prompt=prompt_template)
        
        try:
            response = chain.run({
                "context": context,
                "question": question
            })
            
            # æ·»åŠ æ¥æºä¿¡æ¯
            sources = list(set([doc.metadata.get('filename', 'æœªçŸ¥') for doc in relevant_docs]))
            response += f"\n\nğŸ“š å‚è€ƒæ¥æº: {', '.join(sources)}"
            
            return response
        except Exception as e:
            return f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}"
    
    def list_documents(self):
        """åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        if not self.documents:
            print("ğŸ“š çŸ¥è¯†åº“ä¸ºç©º")
            return
        
        # ç»Ÿè®¡æ–‡æ¡£æ¥æº
        sources = {}
        for doc in self.documents:
            filename = doc.metadata.get('filename', 'æœªçŸ¥')
            sources[filename] = sources.get(filename, 0) + 1
        
        print("ğŸ“š çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨:")
        for filename, count in sources.items():
            print(f"  - {filename}: {count} ä¸ªç‰‡æ®µ")
        
        print(f"\nğŸ“Š æ€»è®¡: {len(sources)} ä¸ªæ–‡ä»¶, {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    
    def clear_knowledge_base(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        self.documents = []
        self.vectorstore = None
        
        # åˆ é™¤æœ¬åœ°å­˜å‚¨çš„æ–‡ä»¶
        try:
            import shutil
            if self.storage_dir.exists():
                shutil.rmtree(self.storage_dir)
                self.storage_dir.mkdir(exist_ok=True)
            print("ğŸ—‘ï¸ çŸ¥è¯†åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {e}")

def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ“š åŸºäºæ–‡æ¡£çš„çŸ¥è¯†åº“å·¥å…·")
    print("=" * 50)
    
    kb = DocumentKnowledgeBase()
    
    while True:
        print("\nå¯ç”¨å‘½ä»¤:")
        print("1. add - æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“")
        print("2. query - åŸºäºçŸ¥è¯†åº“é—®ç­”")
        print("3. list - æŸ¥çœ‹çŸ¥è¯†åº“æ–‡æ¡£")
        print("4. clear - æ¸…ç©ºçŸ¥è¯†åº“")
        print("5. exit - é€€å‡ºç¨‹åº")
        
        command = input("\nè¯·é€‰æ‹©å‘½ä»¤ (1-5): ").strip()
        
        if command == "1" or command.lower() == "add":
            file_paths = input("è¯·è¾“å…¥æ–‡æ¡£è·¯å¾„ (å¤šä¸ªæ–‡ä»¶ç”¨é€—å·åˆ†éš”): ").strip()
            if file_paths:
                paths = [path.strip() for path in file_paths.split(",")]
                kb.add_documents(paths)
        
        elif command == "2" or command.lower() == "query":
            question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            if question:
                print("\nğŸ¤” æ­£åœ¨æ€è€ƒ...")
                response = kb.query(question)
                print(f"\nğŸ’¡ å›ç­”:\n{response}")
        
        elif command == "3" or command.lower() == "list":
            kb.list_documents()
        
        elif command == "4" or command.lower() == "clear":
            confirm = input("ç¡®å®šè¦æ¸…ç©ºçŸ¥è¯†åº“å—? (y/n): ").strip().lower()
            if confirm == 'y':
                kb.clear_knowledge_base()
        
        elif command == "5" or command.lower() == "exit":
            print("ğŸ‘‹ å†è§!")
            break
        
        else:
            print("âŒ æ— æ•ˆå‘½ä»¤ï¼Œè¯·é‡æ–°é€‰æ‹©")

if __name__ == "__main__":
    main()
